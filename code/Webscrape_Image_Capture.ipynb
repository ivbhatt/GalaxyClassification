{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Webscrape_Image_Capture.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BikIvkQJQuGR","executionInfo":{"status":"ok","timestamp":1617924145233,"user_tz":240,"elapsed":5670,"user":{"displayName":"Meghana Kota","photoUrl":"","userId":"05144515559827136900"}},"outputId":"f1f6c610-29e3-4d02-847d-67b334825a4f"},"source":["# Required packages installation\n","!pip install proxycrawl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting proxycrawl\n","  Downloading https://files.pythonhosted.org/packages/4d/a6/6f8af2a99f14996b2ee0a89bc910a84b703b79dc4748cf8b2f27d5426863/proxycrawl-3.1.0-py3-none-any.whl\n","Installing collected packages: proxycrawl\n","Successfully installed proxycrawl-3.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2uqLT2yRWKVt"},"source":["# Importing all the required libraries\n","import os\n","import requests\n","from bs4 import BeautifulSoup\n","from proxycrawl.proxycrawl_api import ProxyCrawlAPI"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5E6JKJTQQiVx","executionInfo":{"status":"ok","timestamp":1617207661039,"user_tz":240,"elapsed":19500,"user":{"displayName":"Ishan Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdO_BIe53vmiEeiy-uRhICGe1y8xUz10i5P3Or=s64","userId":"12876444916696410572"}},"outputId":"c9fe83ee-6904-40a1-8a54-9650a62c3007"},"source":["# mounting on Google Drive,below 2 lines helps in getting the authorization code by loggin into your Google account\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Be1TqSAlTm0T"},"source":["OUTPUT_FOLDER = '/gdrive/Shareddrives/ALDA_Project/data/web_scraping_meghana'\n","OUTPUT_CLASS = \"irregular\"\n","\n","# Create a dictionary with the image type and the number of images you would like to download\n","MASTER_DICT = {\"Irregular galaxies\" : 50,\n","               \"Hubble Irregular galaxies \": 25,\n","               \"Faulkes Irregular galaxies \": 25,\n","               \"NASA Irregular galaxies\": 25,\n","               \"ESA Irregular galaxies\": 25,\n","               \"ISRO Irregular galaxies\": 25,\n","               \"JAXA Irregular galaxies\": 25,\n","               \"CNSA Irregular galaxies\": 25,\n","               \"ROSCOSMOS Irregular galaxies\": 25\n","               }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGaGL0tAQmOa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617209426272,"user_tz":240,"elapsed":237452,"user":{"displayName":"Ishan Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdO_BIe53vmiEeiy-uRhICGe1y8xUz10i5P3Or=s64","userId":"12876444916696410572"}},"outputId":"ab575062-adb9-4dbe-f487-f60944111ac5"},"source":["\"\"\"\n","  This is a web-scraping script designed to gather images of irregular galaxies which were not\n","  included in the Sloan Digitial Sky Survey (SDSS) database.\n","\"\"\"\n","Google_Image = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n","\n","global_count = 0\n","for search_term in MASTER_DICT.keys():\n","  \n","  print(\"Processing.... Search Term:\", search_term)\n","\n","  num_images = MASTER_DICT[search_term]\n","\n","  # Creating the query for the image\n","  print('Searching Images....')\n","  search_url = Google_Image + 'q=' + search_term\n","  api = ProxyCrawlAPI({'token':'N5k48Zl5M9jWzp5SD-PeOg', \"timeout\": 600})\n","  response = api.get(search_url, {'scroll': 'true', 'scroll_interval': '60', 'ajax_wait': 'true'})\n","  #we are looking for resonses with status code 200, which is implication of success value\n","  if response['status_code'] == 200:\n","      b_soup = BeautifulSoup(response['body'], 'html.parser') \n","      results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n","      \n","      count = 0\n","      imagelinks= [] # Create array to hold image links\n","      for res in results:\n","          try:\n","              link = res['data-src']\n","              imagelinks.append(link)\n","              count = count + 1\n","              if(count % 50 == 0):\n","                print(str(count) + \" / \" + str(num_images) + \" found.\")\n","              if (count >= num_images):\n","                  break\n","              \n","          except KeyError:\n","              continue\n","      print(f'Found {len(imagelinks)} images')\n","      print('Start downloading...')\n","      \n","      # Use a request to download the image\n","      for i, imagelink in enumerate(imagelinks):\n","          response = requests.get(imagelink)\n","          \n","          # Open each image link and save the file\n","          imagename =  os.path.join(OUTPUT_FOLDER, OUTPUT_CLASS, OUTPUT_CLASS +\"_\"+ str(global_count+1)+\".jpg\")\n","          global_count += 1\n","          with open(imagename, 'wb') as file:\n","              file.write(response.content)\n","\n","          if((i+1) % 50 == 0):\n","            print(str(i+1) + \" / \" + str(num_images) + \" downloaded.\")\n","\n","      print('Download Completed!')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing.... Search Term: Irregular galaxies\n","Searching Images....\n","50 / 50 found.\n","Found 50 images\n","Start downloading...\n","50 / 50 downloaded.\n","Download Completed!\n","Processing.... Search Term: Hubble Irregular galaxies \n","Searching Images....\n","Found 25 images\n","Start downloading...\n","Download Completed!\n","Processing.... Search Term: Faulkes Irregular galaxies \n","Searching Images....\n","Found 25 images\n","Start downloading...\n","Download Completed!\n","Processing.... Search Term: NASA Irregular galaxies\n","Searching Images....\n","Found 25 images\n","Start downloading...\n","Download Completed!\n","Processing.... Search Term: ESA Irregular galaxies\n","Searching Images....\n","Found 25 images\n","Start downloading...\n","Download Completed!\n","Processing.... Search Term: ISRO Irregular galaxies\n","Searching Images....\n","Found 25 images\n","Start downloading...\n","Download Completed!\n","Processing.... Search Term: JAXA Irregular galaxies\n","Searching Images....\n","Found 16 images\n","Start downloading...\n","Download Completed!\n","Processing.... Search Term: CNSA Irregular galaxies\n","Searching Images....\n","Found 6 images\n","Start downloading...\n","Download Completed!\n","Processing.... Search Term: ROSCOSMOS Irregular galaxies\n","Searching Images....\n","Found 17 images\n","Start downloading...\n","Download Completed!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hNohk270YWMq"},"source":["#specify path\n","OUTPUT_FOLDER = '/gdrive/Shareddrives/ALDA_Project/data/web_scraping_meghana'\n","OUTPUT_CLASS = \"invalid\"\n","\n","MASTER_DICT = {\"cartoon\" : 50,\n","               \"cars\": 50,\n","               \"fruits\" : 50,\n","               \"electronics\": 50,\n","               \"money\" : 50,\n","               \"travel\": 50,\n","               \"people\" : 50,\n","               \"pens\": 50,\n","               \"schools\" : 50,\n","               \"pills\": 50,\n","               \"trees\" : 50,\n","               \"bikes\": 50,\n","               \"beds\" : 50,\n","               \"random\": 50,\n","               \"numbers\" : 50,\n","               \"choclates\": 50,\n","               \"flags\" : 50,\n","               \"houses\": 50,\n","               \"animals\" : 50,\n","               \"food\": 50\n","               }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xubM32jaPN5E"},"source":["\"\"\"\n","  In addition, we also gather images to build the invalid image dataset.\n","\"\"\"\n","Google_Image = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n","\n","global_count = 0\n","for search_term in MASTER_DICT.keys():\n","  \n","  print(\"Processing.... Search Term:\", search_term)\n","\n","  num_images = MASTER_DICT[search_term]\n","\n","  # Creating the query for the image\n","  print('Searching Images....')\n","  search_url = Google_Image + 'q=' + search_term\n","  api = ProxyCrawlAPI({'token':'N5k48Zl5M9jWzp5SD-PeOg', \"timeout\": 600})\n","  response = api.get(search_url, {'scroll': 'true', 'scroll_interval': '60', 'ajax_wait': 'true'})\n","  \n","  if response['status_code'] == 200:\n","      b_soup = BeautifulSoup(response['body'], 'html.parser') \n","      results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n","      \n","      count = 0\n","      imagelinks= [] # Create array to hold image links\n","      for res in results:\n","          try:\n","              link = res['data-src']\n","              imagelinks.append(link)\n","              count = count + 1\n","              if(count % 50 == 0):\n","                print(str(count) + \" / \" + str(num_images) + \" found.\")\n","              if (count >= num_images):\n","                  break\n","              \n","          except KeyError:\n","              continue\n","      print(f'Found {len(imagelinks)} images')\n","      print('Start downloading...')\n","      \n","      # Use a request to download the image\n","      for i, imagelink in enumerate(imagelinks):\n","          response = requests.get(imagelink)\n","\n","          # Open each image link and save the file\n","          imagename =  os.path.join(OUTPUT_FOLDER, OUTPUT_CLASS, OUTPUT_CLASS +\"_\"+ str(global_count+1)+\".jpg\")\n","          global_count += 1\n","          with open(imagename, 'wb') as file:\n","              file.write(response.content)\n","\n","          if((i+1) % 50 == 0):\n","            print(str(i+1) + \" / \" + str(num_images) + \" downloaded.\")\n","\n","      print('Download Completed!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9I2rvQERbbD"},"source":["#specify path\n","OUTPUT_FOLDER = '/gdrive/Shareddrives/ALDA_Project/data/web_scraping_meghana'\n","OUTPUT_CLASS = \"invalid\"\n","\n","MASTER_DICT = {\"random\": 50,\n","               \"numbers\" : 50,\n","               \"choclates\": 50,\n","               \"flags\" : 50,\n","               \"houses\": 50,\n","               \"animals\" : 50,\n","               \"food\": 50,\n","               }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2j21-YXxJiv","executionInfo":{"status":"ok","timestamp":1617220754575,"user_tz":240,"elapsed":1565480,"user":{"displayName":"Ishan Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdO_BIe53vmiEeiy-uRhICGe1y8xUz10i5P3Or=s64","userId":"12876444916696410572"}},"outputId":"05427671-a74b-4510-9e00-4b1bc089da87"},"source":["\"\"\"\n","  Gathering additional invalid images.\n","\"\"\"\n","\n","Google_Image = 'https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&'\n","\n","global_count = 529\n","for search_term in MASTER_DICT.keys():\n","  \n","  print(\"Processing.... Search Term:\", search_term)\n","\n","  num_images = MASTER_DICT[search_term]\n","\n","  # Creating the query for the image\n","  print('Searching Images....')\n","  search_url = Google_Image + 'q=' + search_term\n","  api = ProxyCrawlAPI({'token':'N5k48Zl5M9jWzp5SD-PeOg', \"timeout\": 600})\n","  response = api.get(search_url, {'scroll': 'true', 'scroll_interval': '60', 'ajax_wait': 'true'})\n","  \n","  if response['status_code'] == 200:\n","      b_soup = BeautifulSoup(response['body'], 'html.parser') \n","      results = b_soup.findAll('img', {'class': 'rg_i Q4LuWd'})\n","      \n","      count = 0\n","      imagelinks= [] # Create array to hold image links\n","      for res in results:\n","          try:\n","              link = res['data-src']\n","              imagelinks.append(link)\n","              count = count + 1\n","              if(count % 50 == 0):\n","                print(str(count) + \" / \" + str(num_images) + \" found.\")\n","              if (count >= num_images):\n","                  break\n","              \n","          except KeyError:\n","              continue\n","      print(f'Found {len(imagelinks)} images')\n","      print('Start downloading...')\n","      \n","      # Use a request to download the image\n","      for i, imagelink in enumerate(imagelinks):\n","          response = requests.get(imagelink)\n","\n","          # Open each image link and save the file\n","          imagename =  os.path.join(OUTPUT_FOLDER, OUTPUT_CLASS, OUTPUT_CLASS +\"_\"+ str(global_count+1)+\".jpg\")\n","          global_count += 1\n","          with open(imagename, 'wb') as file:\n","              file.write(response.content)\n","\n","          if((i+1) % 50 == 0):\n","            print(str(i+1) + \" / \" + str(num_images) + \" downloaded.\")\n","\n","      print('Download Completed!')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing.... Search Term: random\n","Searching Images....\n","50 / 50 found.\n","Found 50 images\n","Start downloading...\n","50 / 50 downloaded.\n","Download Completed!\n","Processing.... Search Term: numbers\n","Searching Images....\n","50 / 50 found.\n","Found 50 images\n","Start downloading...\n","50 / 50 downloaded.\n","Download Completed!\n","Processing.... Search Term: choclates\n","Searching Images....\n","50 / 50 found.\n","Found 50 images\n","Start downloading...\n","50 / 50 downloaded.\n","Download Completed!\n","Processing.... Search Term: flags\n","Searching Images....\n","50 / 50 found.\n","Found 50 images\n","Start downloading...\n","50 / 50 downloaded.\n","Download Completed!\n","Processing.... Search Term: houses\n","Searching Images....\n","50 / 50 found.\n","Found 50 images\n","Start downloading...\n","50 / 50 downloaded.\n","Download Completed!\n","Processing.... Search Term: animals\n","Searching Images....\n","50 / 50 found.\n","Found 50 images\n","Start downloading...\n","50 / 50 downloaded.\n","Download Completed!\n","Processing.... Search Term: food\n","Searching Images....\n"],"name":"stdout"}]}]}